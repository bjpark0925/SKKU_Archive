{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ft-Qfs5FpwKA","outputId":"c6eb98c2-8997-4943-99dd-a7bf46a65217"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uf4OimfZpiDC","outputId":"975218dc-2dad-42ac-e1ec-17229fd50e97"},"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","    270/Unknown - 13557s 50s/step - loss: 0.3884 - accuracy: 0.8204"]}],"source":["import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from transformers import InputExample, InputFeatures\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# BERT 토크나이저 및 모델 로드\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","\n","# IMDB 데이터셋 로드\n","dataset = tf.keras.utils.get_file(\n","    fname=\"aclImdb_v1.tar.gz\",\n","    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n","    extract=True,\n",")\n","\n","# 데이터셋을 데이터프레임으로 변환\n","import os\n","import glob\n","\n","def load_imdb_dataset(extract_path):\n","    reviews = []\n","    labels = []\n","    for label in ['pos', 'neg']:\n","        path = os.path.join(os.path.dirname(extract_path), 'aclImdb', 'train', label)\n","        for filename in glob.glob(os.path.join(path, '*.txt')):\n","            with open(filename, 'r', encoding='utf-8') as file:\n","                reviews.append(file.read())\n","                labels.append(0 if label == 'neg' else 1)\n","    return pd.DataFrame({'review': reviews, 'label': labels})\n","\n","df = load_imdb_dataset(dataset)\n","\n","# 훈련 및 검증 데이터셋으로 분할\n","train_df, val_df = train_test_split(df, test_size=0.2)\n","\n","# 데이터셋을 BERT가 이해할 수 있는 형태로 변환\n","def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n","    train_InputExamples = train.apply(lambda x: InputExample(guid=None,\n","                                                            text_a = x[DATA_COLUMN],\n","                                                            text_b = None,\n","                                                            label = x[LABEL_COLUMN]), axis = 1)\n","    validation_InputExamples = test.apply(lambda x: InputExample(guid=None,\n","                                                            text_a = x[DATA_COLUMN],\n","                                                            text_b = None,\n","                                                            label = x[LABEL_COLUMN]), axis = 1)\n","    return train_InputExamples, validation_InputExamples\n","\n","def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n","    features = []\n","\n","    for e in examples:\n","        input_dict = tokenizer.encode_plus(\n","            e.text_a,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","            pad_to_max_length=True,\n","            truncation=True\n","        )\n","        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n","            input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n","            )\n","        )\n","\n","    def gen():\n","        for f in features:\n","            yield (\n","                {\n","                    \"input_ids\": f.input_ids,\n","                    \"attention_mask\": f.attention_mask,\n","                    \"token_type_ids\": f.token_type_ids,\n","                },\n","                f.label,\n","            )\n","\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n","        (\n","            {\n","                \"input_ids\": tf.TensorShape([None]),\n","                \"attention_mask\": tf.TensorShape([None]),\n","                \"token_type_ids\": tf.TensorShape([None]),\n","            },\n","            tf.TensorShape([]),\n","        ),\n","    )\n","\n","DATA_COLUMN = 'review'\n","LABEL_COLUMN = 'label'\n","\n","train_InputExamples, validation_InputExamples = convert_data_to_examples(train_df, val_df, DATA_COLUMN, LABEL_COLUMN)\n","\n","train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n","train_data = train_data.shuffle(100).batch(32).repeat(2)\n","\n","validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n","validation_data = validation_data.batch(32)\n","\n","# 모델 컴파일\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n","\n","# 모델 훈련\n","history = model.fit(train_data, epochs=2, validation_data=validation_data)\n","\n","# 훈련 및 검증 손실 및 정확도 시각화\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}