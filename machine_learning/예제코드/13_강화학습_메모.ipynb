{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrkauP7GBovdxzPFxPiSUl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://incredible.ai/machine-learning/2019/02/10/Q-Learning/  \n","https://lee-jaewon.github.io/reinforcement_learning/RL_intro_3/"],"metadata":{"id":"PdQf2ZLYYx4U"}},{"cell_type":"code","source":["env = gym.make('MountainCar-v0')\n","env.observation_space.high  # array([0.6 , 0.07], dtype=float32)\n","env.observation_space.low   # array([-1.2 , -0.07], dtype=float32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qnSMkDKGT9aG","executionInfo":{"status":"ok","timestamp":1700896169774,"user_tz":-540,"elapsed":356,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}},"outputId":"84e7fea2-f169-489d-a0a6-4d83910ebb0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.2 , -0.07], dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["env = gym.make('MountainCar-v0')\n","n_state = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n","n_state = np.round(n_state, 0).astype(int) + 1\n","\n","Q = np.random.uniform(-1, 1, size=(n_state[0], n_state[1], env.action_space.n))\n","print('Q shape:', Q.shape)\n","print('Q Table')\n","print(Q[1:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lZEwOXDUBga","executionInfo":{"status":"ok","timestamp":1700896187551,"user_tz":-540,"elapsed":328,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}},"outputId":"500b33e9-a89d-4177-8f0c-e0dd529aeeac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Q shape: (19, 15, 3)\n","Q Table\n","[[[ 0.5598881   0.55363559  0.56942304]\n","  [-0.46180549  0.63332862  0.42495505]\n","  [ 0.12902945 -0.09786553  0.22399982]\n","  [ 0.14215785  0.66569276 -0.4691401 ]\n","  [ 0.34137618  0.3086423   0.50699602]\n","  [ 0.02582433 -0.21722055  0.70917064]\n","  [-0.78174958 -0.44021225 -0.72401939]\n","  [-0.91302107 -0.89274312 -0.0651827 ]\n","  [-0.51054098  0.17135171  0.14322992]\n","  [-0.33646288 -0.72559437  0.9739868 ]\n","  [ 0.68338796  0.51602966  0.7471864 ]\n","  [-0.42211721  0.76165745  0.78417915]\n","  [-0.43535923 -0.54087993 -0.13789801]\n","  [ 0.26094848 -0.45425332  0.75604422]\n","  [ 0.76369095  0.91039326  0.85233833]]]\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm_notebook\n","def discretize(env, state):\n","    state = (state - env.observation_space.low) * np.array([10, 100])\n","    state = np.round(state, 0).astype(int)\n","    return state\n","\n","def train(env, Q, epochs=10000, lr=0.1, gamma=0.9, epsilon=0.9):\n","    reduction = epsilon/epochs\n","    action_n = env.action_space.n\n","\n","    rewards = list()\n","\n","    for epoch in tqdm_notebook(range(epochs)):\n","        state = env.reset()\n","        state = discretize(env, state)\n","\n","        done = False\n","        _tot_reward = 0\n","        _tot_rand_action = 0\n","        _tot_q_action = 0\n","        _max_pos = 0\n","\n","        while not done:\n","\n","            # Calculate next action\n","            if np.random.random() < 1 - epsilon:\n","                action = np.argmax(Q[state[0], state[1]])\n","                _tot_q_action += 1\n","            else:\n","                action = np.random.randint(0, action_n)\n","                _tot_rand_action += 1\n","\n","            # Step!\n","            next_state, reward, done, info = env.step(action)\n","            next_state_apx = discretize(env, next_state)\n","\n","            # Terminal Update\n","            if done and next_state[0] >= 0.5:\n","                Q[next_state_apx[0], next_state_apx[1], action] = reward\n","            else:\n","                delta = lr * (reward + gamma * np.max(Q[next_state_apx[0], next_state_apx[1]]) -\n","                              Q[state[0], state[1], action])\n","                Q[state[0], state[1], action] += delta\n","\n","            state = next_state_apx\n","            _tot_reward += reward\n","\n","        # Decay Epsilon\n","        if epsilon > 0:\n","            epsilon -= reduction\n","            epsilon = round(epsilon, 4)\n","\n","        # Track Rewards\n","        rewards.append(_tot_reward)\n","\n","        # Log\n","        if epoch%100 == 0:\n","            print(f'\\repoch:{epoch} | tot reward:{_tot_reward} | epsilon:{epsilon} | '\n","                  f'rand action:{_tot_rand_action} | Q action:{_tot_q_action}')\n","\n","train(env, Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9a3255c6533d49698f3289626927d617","f55ccdcf1cfc4d89a5fc25e22b0e2c2c","0f59e92e055248629001492394b17765","c26a2e4e550642b991b75fde2971169c","9259b138ac5c4bcca1ae3ea209a32415","6195187e276a41aa907e263b9b77f9c3","bc5861885c9c47e58df2fdc49f471c35","4389a4c1aa9f4ed29d27d4eb5e02c48e","04b190e517644d27b1aa5a503ed34850","bfe4fe25736b486d818210c11448e3a7","29eae87af63649b2988068973cfafb77"]},"id":"mfsJm3DuUGES","executionInfo":{"status":"ok","timestamp":1700896462516,"user_tz":-540,"elapsed":198818,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}},"outputId":"3ffc4e5f-6119-4a44-f46d-518980e3d54f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-f57416b1c6e8>:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for epoch in tqdm_notebook(range(epochs)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3255c6533d49698f3289626927d617"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["epoch:0 | tot reward:-200.0 | epsilon:0.8999 | rand action:185 | Q action:15\n","epoch:100 | tot reward:-200.0 | epsilon:0.8899 | rand action:176 | Q action:24\n","epoch:200 | tot reward:-200.0 | epsilon:0.8799 | rand action:178 | Q action:22\n","epoch:300 | tot reward:-200.0 | epsilon:0.8699 | rand action:169 | Q action:31\n","epoch:400 | tot reward:-200.0 | epsilon:0.8599 | rand action:173 | Q action:27\n","epoch:500 | tot reward:-200.0 | epsilon:0.8499 | rand action:168 | Q action:32\n","epoch:600 | tot reward:-200.0 | epsilon:0.8399 | rand action:168 | Q action:32\n","epoch:700 | tot reward:-200.0 | epsilon:0.8299 | rand action:166 | Q action:34\n","epoch:800 | tot reward:-200.0 | epsilon:0.8199 | rand action:161 | Q action:39\n","epoch:900 | tot reward:-200.0 | epsilon:0.8099 | rand action:169 | Q action:31\n","epoch:1000 | tot reward:-200.0 | epsilon:0.7999 | rand action:163 | Q action:37\n","epoch:1100 | tot reward:-200.0 | epsilon:0.7899 | rand action:146 | Q action:54\n","epoch:1200 | tot reward:-200.0 | epsilon:0.7799 | rand action:151 | Q action:49\n","epoch:1300 | tot reward:-200.0 | epsilon:0.7699 | rand action:142 | Q action:58\n","epoch:1400 | tot reward:-200.0 | epsilon:0.7599 | rand action:153 | Q action:47\n","epoch:1500 | tot reward:-200.0 | epsilon:0.7499 | rand action:153 | Q action:47\n","epoch:1600 | tot reward:-200.0 | epsilon:0.7399 | rand action:159 | Q action:41\n","epoch:1700 | tot reward:-200.0 | epsilon:0.7299 | rand action:144 | Q action:56\n","epoch:1800 | tot reward:-200.0 | epsilon:0.7199 | rand action:144 | Q action:56\n","epoch:1900 | tot reward:-200.0 | epsilon:0.7099 | rand action:142 | Q action:58\n","epoch:2000 | tot reward:-200.0 | epsilon:0.6999 | rand action:144 | Q action:56\n","epoch:2100 | tot reward:-200.0 | epsilon:0.6899 | rand action:131 | Q action:69\n","epoch:2200 | tot reward:-200.0 | epsilon:0.6799 | rand action:140 | Q action:60\n","epoch:2300 | tot reward:-200.0 | epsilon:0.6699 | rand action:123 | Q action:77\n","epoch:2400 | tot reward:-200.0 | epsilon:0.6599 | rand action:134 | Q action:66\n","epoch:2500 | tot reward:-200.0 | epsilon:0.6499 | rand action:129 | Q action:71\n","epoch:2600 | tot reward:-200.0 | epsilon:0.6399 | rand action:118 | Q action:82\n","epoch:2700 | tot reward:-200.0 | epsilon:0.6299 | rand action:128 | Q action:72\n","epoch:2800 | tot reward:-200.0 | epsilon:0.6199 | rand action:131 | Q action:69\n","epoch:2900 | tot reward:-200.0 | epsilon:0.6099 | rand action:119 | Q action:81\n","epoch:3000 | tot reward:-200.0 | epsilon:0.5999 | rand action:119 | Q action:81\n","epoch:3100 | tot reward:-200.0 | epsilon:0.5899 | rand action:117 | Q action:83\n","epoch:3200 | tot reward:-200.0 | epsilon:0.5799 | rand action:113 | Q action:87\n","epoch:3300 | tot reward:-200.0 | epsilon:0.5699 | rand action:92 | Q action:108\n","epoch:3400 | tot reward:-200.0 | epsilon:0.5599 | rand action:110 | Q action:90\n","epoch:3500 | tot reward:-200.0 | epsilon:0.5499 | rand action:106 | Q action:94\n","epoch:3600 | tot reward:-200.0 | epsilon:0.5399 | rand action:108 | Q action:92\n","epoch:3700 | tot reward:-200.0 | epsilon:0.5299 | rand action:106 | Q action:94\n","epoch:3800 | tot reward:-200.0 | epsilon:0.5199 | rand action:91 | Q action:109\n","epoch:3900 | tot reward:-200.0 | epsilon:0.5099 | rand action:112 | Q action:88\n","epoch:4000 | tot reward:-200.0 | epsilon:0.4999 | rand action:99 | Q action:101\n","epoch:4100 | tot reward:-200.0 | epsilon:0.4899 | rand action:115 | Q action:85\n","epoch:4200 | tot reward:-200.0 | epsilon:0.4799 | rand action:101 | Q action:99\n","epoch:4300 | tot reward:-200.0 | epsilon:0.4699 | rand action:98 | Q action:102\n","epoch:4400 | tot reward:-200.0 | epsilon:0.4599 | rand action:96 | Q action:104\n","epoch:4500 | tot reward:-200.0 | epsilon:0.4499 | rand action:97 | Q action:103\n","epoch:4600 | tot reward:-200.0 | epsilon:0.4399 | rand action:92 | Q action:108\n","epoch:4700 | tot reward:-200.0 | epsilon:0.4299 | rand action:81 | Q action:119\n","epoch:4800 | tot reward:-200.0 | epsilon:0.4199 | rand action:87 | Q action:113\n","epoch:4900 | tot reward:-200.0 | epsilon:0.4099 | rand action:86 | Q action:114\n","epoch:5000 | tot reward:-200.0 | epsilon:0.3999 | rand action:73 | Q action:127\n","epoch:5100 | tot reward:-200.0 | epsilon:0.3899 | rand action:86 | Q action:114\n","epoch:5200 | tot reward:-200.0 | epsilon:0.3799 | rand action:69 | Q action:131\n","epoch:5300 | tot reward:-200.0 | epsilon:0.3699 | rand action:74 | Q action:126\n","epoch:5400 | tot reward:-200.0 | epsilon:0.3599 | rand action:75 | Q action:125\n","epoch:5500 | tot reward:-200.0 | epsilon:0.3499 | rand action:84 | Q action:116\n","epoch:5600 | tot reward:-200.0 | epsilon:0.3399 | rand action:82 | Q action:118\n","epoch:5700 | tot reward:-200.0 | epsilon:0.3299 | rand action:56 | Q action:144\n","epoch:5800 | tot reward:-200.0 | epsilon:0.3199 | rand action:69 | Q action:131\n","epoch:5900 | tot reward:-200.0 | epsilon:0.3099 | rand action:60 | Q action:140\n","epoch:6000 | tot reward:-200.0 | epsilon:0.2999 | rand action:59 | Q action:141\n","epoch:6100 | tot reward:-200.0 | epsilon:0.2899 | rand action:63 | Q action:137\n","epoch:6200 | tot reward:-200.0 | epsilon:0.2799 | rand action:59 | Q action:141\n","epoch:6300 | tot reward:-200.0 | epsilon:0.2699 | rand action:58 | Q action:142\n","epoch:6400 | tot reward:-200.0 | epsilon:0.2599 | rand action:42 | Q action:158\n","epoch:6500 | tot reward:-200.0 | epsilon:0.2499 | rand action:46 | Q action:154\n","epoch:6600 | tot reward:-150.0 | epsilon:0.2399 | rand action:33 | Q action:117\n","epoch:6700 | tot reward:-200.0 | epsilon:0.2299 | rand action:55 | Q action:145\n","epoch:6800 | tot reward:-200.0 | epsilon:0.2199 | rand action:48 | Q action:152\n","epoch:6900 | tot reward:-200.0 | epsilon:0.2099 | rand action:49 | Q action:151\n","epoch:7000 | tot reward:-200.0 | epsilon:0.1999 | rand action:40 | Q action:160\n","epoch:7100 | tot reward:-200.0 | epsilon:0.1899 | rand action:34 | Q action:166\n","epoch:7200 | tot reward:-200.0 | epsilon:0.1799 | rand action:25 | Q action:175\n","epoch:7300 | tot reward:-162.0 | epsilon:0.1699 | rand action:30 | Q action:132\n","epoch:7400 | tot reward:-195.0 | epsilon:0.1599 | rand action:25 | Q action:170\n","epoch:7500 | tot reward:-200.0 | epsilon:0.1499 | rand action:29 | Q action:171\n","epoch:7600 | tot reward:-200.0 | epsilon:0.1399 | rand action:25 | Q action:175\n","epoch:7700 | tot reward:-200.0 | epsilon:0.1299 | rand action:23 | Q action:177\n","epoch:7800 | tot reward:-149.0 | epsilon:0.1199 | rand action:10 | Q action:139\n","epoch:7900 | tot reward:-200.0 | epsilon:0.1099 | rand action:15 | Q action:185\n","epoch:8000 | tot reward:-153.0 | epsilon:0.0999 | rand action:10 | Q action:143\n","epoch:8100 | tot reward:-200.0 | epsilon:0.0899 | rand action:19 | Q action:181\n","epoch:8200 | tot reward:-156.0 | epsilon:0.0799 | rand action:9 | Q action:147\n","epoch:8300 | tot reward:-157.0 | epsilon:0.0699 | rand action:8 | Q action:149\n","epoch:8400 | tot reward:-160.0 | epsilon:0.0599 | rand action:15 | Q action:145\n","epoch:8500 | tot reward:-200.0 | epsilon:0.0499 | rand action:10 | Q action:190\n","epoch:8600 | tot reward:-200.0 | epsilon:0.0399 | rand action:6 | Q action:194\n","epoch:8700 | tot reward:-200.0 | epsilon:0.0299 | rand action:7 | Q action:193\n","epoch:8800 | tot reward:-154.0 | epsilon:0.0199 | rand action:6 | Q action:148\n","epoch:8900 | tot reward:-200.0 | epsilon:0.0099 | rand action:3 | Q action:197\n","epoch:9000 | tot reward:-159.0 | epsilon:0.0 | rand action:0 | Q action:159\n","epoch:9100 | tot reward:-200.0 | epsilon:0.0 | rand action:0 | Q action:200\n","epoch:9200 | tot reward:-161.0 | epsilon:0.0 | rand action:0 | Q action:161\n","epoch:9300 | tot reward:-153.0 | epsilon:0.0 | rand action:0 | Q action:153\n","epoch:9400 | tot reward:-154.0 | epsilon:0.0 | rand action:0 | Q action:154\n","epoch:9500 | tot reward:-200.0 | epsilon:0.0 | rand action:0 | Q action:200\n","epoch:9600 | tot reward:-142.0 | epsilon:0.0 | rand action:0 | Q action:142\n","epoch:9700 | tot reward:-144.0 | epsilon:0.0 | rand action:0 | Q action:144\n","epoch:9800 | tot reward:-194.0 | epsilon:0.0 | rand action:0 | Q action:194\n","epoch:9900 | tot reward:-155.0 | epsilon:0.0 | rand action:0 | Q action:155\n"]}]},{"cell_type":"code","source":["env = gym.make('MountainCar-v0')\n","state = env.reset()\n","state = discretize(env, state)\n","\n","env.render()\n","input()\n","\n","while True:\n","    env.render()\n","    action = np.argmax(Q[state[0], state[1]])\n","    state, reward, done, info = env.step(action)\n","    state = discretize(env, state)\n","\n","    print(f'\\rstate:{state} | reward:{reward} | done:{done} | info:{info}')\n","\n","    if done:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RX-X2kl1UcSw","executionInfo":{"status":"ok","timestamp":1700896674979,"user_tz":-540,"elapsed":212474,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}},"outputId":"cb3ba817-a96f-43b1-d9f9-d06e170a55b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n","If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["0.9\n","state:[8 7] | reward:-1.0 | done:False | info:{}\n","state:[8 7] | reward:-1.0 | done:False | info:{}\n","state:[8 6] | reward:-1.0 | done:False | info:{}\n","state:[8 6] | reward:-1.0 | done:False | info:{}\n","state:[8 6] | reward:-1.0 | done:False | info:{}\n","state:[7 6] | reward:-1.0 | done:False | info:{}\n","state:[7 6] | reward:-1.0 | done:False | info:{}\n","state:[7 6] | reward:-1.0 | done:False | info:{}\n","state:[7 6] | reward:-1.0 | done:False | info:{}\n","state:[7 5] | reward:-1.0 | done:False | info:{}\n","state:[7 5] | reward:-1.0 | done:False | info:{}\n","state:[7 5] | reward:-1.0 | done:False | info:{}\n","state:[6 5] | reward:-1.0 | done:False | info:{}\n","state:[6 5] | reward:-1.0 | done:False | info:{}\n","state:[6 5] | reward:-1.0 | done:False | info:{}\n","state:[6 5] | reward:-1.0 | done:False | info:{}\n","state:[6 5] | reward:-1.0 | done:False | info:{}\n","state:[5 5] | reward:-1.0 | done:False | info:{}\n","state:[5 5] | reward:-1.0 | done:False | info:{}\n","state:[5 5] | reward:-1.0 | done:False | info:{}\n","state:[5 5] | reward:-1.0 | done:False | info:{}\n","state:[5 5] | reward:-1.0 | done:False | info:{}\n","state:[4 5] | reward:-1.0 | done:False | info:{}\n","state:[4 5] | reward:-1.0 | done:False | info:{}\n","state:[4 5] | reward:-1.0 | done:False | info:{}\n","state:[4 5] | reward:-1.0 | done:False | info:{}\n","state:[4 5] | reward:-1.0 | done:False | info:{}\n","state:[4 5] | reward:-1.0 | done:False | info:{}\n","state:[3 6] | reward:-1.0 | done:False | info:{}\n","state:[3 6] | reward:-1.0 | done:False | info:{}\n","state:[3 6] | reward:-1.0 | done:False | info:{}\n","state:[3 6] | reward:-1.0 | done:False | info:{}\n","state:[3 6] | reward:-1.0 | done:False | info:{}\n","state:[3 7] | reward:-1.0 | done:False | info:{}\n","state:[3 7] | reward:-1.0 | done:False | info:{}\n","state:[3 7] | reward:-1.0 | done:False | info:{}\n","state:[3 8] | reward:-1.0 | done:False | info:{}\n","state:[3 8] | reward:-1.0 | done:False | info:{}\n","state:[3 8] | reward:-1.0 | done:False | info:{}\n","state:[3 8] | reward:-1.0 | done:False | info:{}\n","state:[3 8] | reward:-1.0 | done:False | info:{}\n","state:[4 8] | reward:-1.0 | done:False | info:{}\n","state:[4 8] | reward:-1.0 | done:False | info:{}\n","state:[4 9] | reward:-1.0 | done:False | info:{}\n","state:[4 9] | reward:-1.0 | done:False | info:{}\n","state:[4 9] | reward:-1.0 | done:False | info:{}\n","state:[4 9] | reward:-1.0 | done:False | info:{}\n","state:[5 9] | reward:-1.0 | done:False | info:{}\n","state:[5 9] | reward:-1.0 | done:False | info:{}\n","state:[5 9] | reward:-1.0 | done:False | info:{}\n","state:[ 5 10] | reward:-1.0 | done:False | info:{}\n","state:[ 6 10] | reward:-1.0 | done:False | info:{}\n","state:[ 6 10] | reward:-1.0 | done:False | info:{}\n","state:[ 6 10] | reward:-1.0 | done:False | info:{}\n","state:[ 6 10] | reward:-1.0 | done:False | info:{}\n","state:[ 7 10] | reward:-1.0 | done:False | info:{}\n","state:[ 7 10] | reward:-1.0 | done:False | info:{}\n","state:[ 7 10] | reward:-1.0 | done:False | info:{}\n","state:[7 9] | reward:-1.0 | done:False | info:{}\n","state:[ 8 10] | reward:-1.0 | done:False | info:{}\n","state:[ 8 10] | reward:-1.0 | done:False | info:{}\n","state:[ 8 10] | reward:-1.0 | done:False | info:{}\n","state:[ 9 10] | reward:-1.0 | done:False | info:{}\n","state:[ 9 10] | reward:-1.0 | done:False | info:{}\n","state:[ 9 10] | reward:-1.0 | done:False | info:{}\n","state:[9 9] | reward:-1.0 | done:False | info:{}\n","state:[10  9] | reward:-1.0 | done:False | info:{}\n","state:[10  9] | reward:-1.0 | done:False | info:{}\n","state:[10  9] | reward:-1.0 | done:False | info:{}\n","state:[10  9] | reward:-1.0 | done:False | info:{}\n","state:[10  9] | reward:-1.0 | done:False | info:{}\n","state:[11  9] | reward:-1.0 | done:False | info:{}\n","state:[11  9] | reward:-1.0 | done:False | info:{}\n","state:[11  9] | reward:-1.0 | done:False | info:{}\n","state:[11  8] | reward:-1.0 | done:False | info:{}\n","state:[11  8] | reward:-1.0 | done:False | info:{}\n","state:[11  8] | reward:-1.0 | done:False | info:{}\n","state:[11  7] | reward:-1.0 | done:False | info:{}\n","state:[11  7] | reward:-1.0 | done:False | info:{}\n","state:[11  7] | reward:-1.0 | done:False | info:{}\n","state:[11  6] | reward:-1.0 | done:False | info:{}\n","state:[11  6] | reward:-1.0 | done:False | info:{}\n","state:[11  6] | reward:-1.0 | done:False | info:{}\n","state:[11  5] | reward:-1.0 | done:False | info:{}\n","state:[11  5] | reward:-1.0 | done:False | info:{}\n","state:[10  5] | reward:-1.0 | done:False | info:{}\n","state:[10  4] | reward:-1.0 | done:False | info:{}\n","state:[10  4] | reward:-1.0 | done:False | info:{}\n","state:[10  4] | reward:-1.0 | done:False | info:{}\n","state:[9 4] | reward:-1.0 | done:False | info:{}\n","state:[9 4] | reward:-1.0 | done:False | info:{}\n","state:[9 3] | reward:-1.0 | done:False | info:{}\n","state:[8 3] | reward:-1.0 | done:False | info:{}\n","state:[8 3] | reward:-1.0 | done:False | info:{}\n","state:[7 3] | reward:-1.0 | done:False | info:{}\n","state:[7 3] | reward:-1.0 | done:False | info:{}\n","state:[7 3] | reward:-1.0 | done:False | info:{}\n","state:[6 3] | reward:-1.0 | done:False | info:{}\n","state:[6 3] | reward:-1.0 | done:False | info:{}\n","state:[6 3] | reward:-1.0 | done:False | info:{}\n","state:[5 3] | reward:-1.0 | done:False | info:{}\n","state:[5 3] | reward:-1.0 | done:False | info:{}\n","state:[4 3] | reward:-1.0 | done:False | info:{}\n","state:[4 3] | reward:-1.0 | done:False | info:{}\n","state:[4 3] | reward:-1.0 | done:False | info:{}\n","state:[3 4] | reward:-1.0 | done:False | info:{}\n","state:[3 4] | reward:-1.0 | done:False | info:{}\n","state:[3 4] | reward:-1.0 | done:False | info:{}\n","state:[2 4] | reward:-1.0 | done:False | info:{}\n","state:[2 4] | reward:-1.0 | done:False | info:{}\n","state:[2 5] | reward:-1.0 | done:False | info:{}\n","state:[2 5] | reward:-1.0 | done:False | info:{}\n","state:[2 5] | reward:-1.0 | done:False | info:{}\n","state:[1 5] | reward:-1.0 | done:False | info:{}\n","state:[1 6] | reward:-1.0 | done:False | info:{}\n","state:[1 6] | reward:-1.0 | done:False | info:{}\n","state:[1 6] | reward:-1.0 | done:False | info:{}\n","state:[1 7] | reward:-1.0 | done:False | info:{}\n","state:[1 7] | reward:-1.0 | done:False | info:{}\n","state:[1 8] | reward:-1.0 | done:False | info:{}\n","state:[1 8] | reward:-1.0 | done:False | info:{}\n","state:[1 8] | reward:-1.0 | done:False | info:{}\n","state:[2 9] | reward:-1.0 | done:False | info:{}\n","state:[2 9] | reward:-1.0 | done:False | info:{}\n","state:[2 9] | reward:-1.0 | done:False | info:{}\n","state:[ 2 10] | reward:-1.0 | done:False | info:{}\n","state:[ 2 10] | reward:-1.0 | done:False | info:{}\n","state:[ 3 10] | reward:-1.0 | done:False | info:{}\n","state:[ 3 11] | reward:-1.0 | done:False | info:{}\n","state:[ 4 11] | reward:-1.0 | done:False | info:{}\n","state:[ 4 11] | reward:-1.0 | done:False | info:{}\n","state:[ 4 11] | reward:-1.0 | done:False | info:{}\n","state:[ 5 12] | reward:-1.0 | done:False | info:{}\n","state:[ 5 12] | reward:-1.0 | done:False | info:{}\n","state:[ 6 12] | reward:-1.0 | done:False | info:{}\n","state:[ 6 12] | reward:-1.0 | done:False | info:{}\n","state:[ 7 12] | reward:-1.0 | done:False | info:{}\n","state:[ 7 12] | reward:-1.0 | done:False | info:{}\n","state:[ 8 12] | reward:-1.0 | done:False | info:{}\n","state:[ 8 12] | reward:-1.0 | done:False | info:{}\n","state:[ 9 12] | reward:-1.0 | done:False | info:{}\n","state:[ 9 12] | reward:-1.0 | done:False | info:{}\n","state:[10 12] | reward:-1.0 | done:False | info:{}\n","state:[10 12] | reward:-1.0 | done:False | info:{}\n","state:[11 11] | reward:-1.0 | done:False | info:{}\n","state:[11 11] | reward:-1.0 | done:False | info:{}\n","state:[12 11] | reward:-1.0 | done:False | info:{}\n","state:[12 11] | reward:-1.0 | done:False | info:{}\n","state:[12 11] | reward:-1.0 | done:False | info:{}\n","state:[13 10] | reward:-1.0 | done:False | info:{}\n","state:[13 10] | reward:-1.0 | done:False | info:{}\n","state:[13 10] | reward:-1.0 | done:False | info:{}\n","state:[13 10] | reward:-1.0 | done:False | info:{}\n","state:[14 10] | reward:-1.0 | done:False | info:{}\n","state:[14 10] | reward:-1.0 | done:False | info:{}\n","state:[14  9] | reward:-1.0 | done:False | info:{}\n","state:[14  9] | reward:-1.0 | done:False | info:{}\n","state:[15  9] | reward:-1.0 | done:False | info:{}\n","state:[15  9] | reward:-1.0 | done:False | info:{}\n","state:[15  9] | reward:-1.0 | done:False | info:{}\n","state:[15  9] | reward:-1.0 | done:False | info:{}\n","state:[15  9] | reward:-1.0 | done:False | info:{}\n","state:[16  9] | reward:-1.0 | done:False | info:{}\n","state:[16  9] | reward:-1.0 | done:False | info:{}\n","state:[16  9] | reward:-1.0 | done:False | info:{}\n","state:[16  9] | reward:-1.0 | done:False | info:{}\n","state:[16  9] | reward:-1.0 | done:False | info:{}\n","state:[17  9] | reward:-1.0 | done:False | info:{}\n","state:[17  9] | reward:-1.0 | done:False | info:{}\n","state:[17  9] | reward:-1.0 | done:True | info:{'TimeLimit.truncated': False}\n"]}]}]}