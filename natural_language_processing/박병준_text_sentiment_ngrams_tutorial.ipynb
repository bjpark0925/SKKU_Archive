{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"U8p5jQvCOSKp","executionInfo":{"status":"ok","timestamp":1698487948037,"user_tz":-540,"elapsed":5,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["# For tips on running notebooks in Google Colab, see\n","# https://pytorch.org/tutorials/beginner/colab\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"G_leJFMtOSKs"},"source":["\n","# Text classification with the torchtext library\n","\n","In this tutorial, we will show how to use the torchtext library to build the dataset for the text classification analysis. Users will have the flexibility to\n","\n","   - Access to the raw data as an iterator\n","   - Build data processing pipeline to convert the raw text strings into ``torch.Tensor`` that can be used to train the model\n","   - Shuffle and iterate the data with [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)_\n","\n","\n","## Prerequisites\n","\n","A recent 2.x version of the ``portalocker`` package needs to be installed prior to running the tutorial.\n","For example, in the Colab environment, this can be done by adding the following line at the top of the script:\n","\n",".. code-block:: bash\n","     \n","    !pip install -U portalocker>=2.0.0`\n"]},{"cell_type":"code","source":["!pip install -U portalocker>=2.0.0"],"metadata":{"id":"GP0ZbtjNOaJk","executionInfo":{"status":"ok","timestamp":1698487953884,"user_tz":-540,"elapsed":5851,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm7rGUCKOSKu"},"source":["### Access to the raw dataset iterators\n","\n","The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the ``AG_NEWS`` dataset iterators yield the raw data as a tuple of label and text.\n","\n","To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hodN9-mgOSKu","executionInfo":{"status":"ok","timestamp":1698487957058,"user_tz":-540,"elapsed":3177,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["import torch\n","from torchtext.datasets import AG_NEWS\n","\n","train_iter = iter(AG_NEWS(split=\"train\"))"]},{"cell_type":"markdown","metadata":{"id":"OzFiSdt5OSKu"},"source":["::\n","\n","    next(train_iter)\n","    >>> (3, \"Fears for T N pension after talks Unions representing workers at Turner\n","    Newall say they are 'disappointed' after talks with stricken parent firm Federal\n","    Mogul.\")\n","\n","    next(train_iter)\n","    >>> (4, \"The Race is On: Second Private Team Sets Launch Date for Human\n","    Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\\\team of\n","    rocketeers competing for the  #36;10 million Ansari X Prize, a contest\n","    for\\\\privately funded suborbital space flight, has officially announced\n","    the first\\\\launch date for its manned rocket.\")\n","\n","    next(train_iter)\n","    >>> (4, 'Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded\n","    by a chemistry researcher at the University of Louisville won a grant to develop\n","    a method of producing better peptides, which are short chains of amino acids, the\n","    building blocks of proteins.')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"thO8IXrFOSKv"},"source":["### Prepare data processing pipelines\n","\n","We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer. Those are the basic data processing building blocks for raw text string.\n","\n","Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in\n","factory function `build_vocab_from_iterator` which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the\n","vocabulary.\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FhQxraLAOSKv","executionInfo":{"status":"ok","timestamp":1698487970997,"user_tz":-540,"elapsed":12863,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer(\"basic_english\")\n","train_iter = AG_NEWS(split=\"train\")\n","\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"]},{"cell_type":"markdown","metadata":{"id":"kmYz3iKKOSKw"},"source":["The vocabulary block converts a list of tokens into integers.\n","\n","::\n","\n","    vocab(['here', 'is', 'an', 'example'])\n","    >>> [475, 21, 30, 5297]\n","\n","Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Gia5iq9XOSKw","executionInfo":{"status":"ok","timestamp":1698487970997,"user_tz":-540,"elapsed":3,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"markdown","metadata":{"id":"G8uNXHGVOSKw"},"source":["The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,\n","\n","::\n","\n","    text_pipeline('here is the an example')\n","    >>> [475, 21, 2, 30, 5297]\n","    label_pipeline('10')\n","    >>> 9\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EP1RIp5fOSKw"},"source":["### Generate data batch and iterator\n","\n","[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)_\n","is recommended for PyTorch users (a tutorial is [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)_).\n","It works with a map-style dataset that implements the ``getitem()`` and ``len()`` protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of ``False``.\n","\n","Before sending to the model, ``collate_fn`` function works on a batch of samples generated from ``DataLoader``. The input to ``collate_fn`` is a batch of data with the batch size in ``DataLoader``, and ``collate_fn`` processes them according to the data processing pipelines declared previously. Pay attention here and make sure that ``collate_fn`` is declared as a top level def. This ensures that the function is available in each worker.\n","\n","In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of ``nn.EmbeddingBag``. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n","\n"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"OtSTZXOWOSKx","executionInfo":{"status":"ok","timestamp":1698501887049,"user_tz":-540,"elapsed":308,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for _label, _text in batch:\n","        label_list.append(label_pipeline(_label))\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","        offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)\n","\n","\n","train_iter = AG_NEWS(split=\"train\")\n","dataloader = DataLoader(\n","    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",")"]},{"cell_type":"markdown","metadata":{"id":"fkMF26kXOSKx"},"source":["### Define the model\n","\n","The model is composed of the [nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag)_ layer plus a linear layer for the classification purpose. ``nn.EmbeddingBag`` with the default mode of \"mean\" computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, ``nn.EmbeddingBag`` module requires no padding here since the text lengths are saved in offsets.\n","\n","Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n","the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n","performance and memory efficiency to process a sequence of tensors.\n","\n","<img src=\"file://../_static/img/text_sentiment_ngrams_model.png\">\n","\n","\n"]},{"cell_type":"code","execution_count":192,"metadata":{"id":"pC9GYUXJOSKx","executionInfo":{"status":"ok","timestamp":1698501893946,"user_tz":-540,"elapsed":516,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["from torch import nn\n","\n","\n","class TextClassificationModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n","        self.fc = nn.Linear(embed_dim, embed_dim)\n","        self.fc = nn.Linear(embed_dim, embed_dim)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)"]},{"cell_type":"markdown","metadata":{"id":"Zlw3wUCWOSKx"},"source":["### Initiate an instance\n","\n","The ``AG_NEWS`` dataset has four labels and therefore the number of classes is four.\n","\n","::\n","\n","   1 : World\n","   2 : Sports\n","   3 : Business\n","   4 : Sci/Tec\n","\n","We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,\n","\n","\n"]},{"cell_type":"code","execution_count":193,"metadata":{"id":"i09AwzzdOSKx","executionInfo":{"status":"ok","timestamp":1698501898772,"user_tz":-540,"elapsed":1612,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["train_iter = AG_NEWS(split=\"train\")\n","num_class = len(set([label for (label, text) in train_iter]))\n","vocab_size = len(vocab)\n","emsize = 32\n","model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"]},{"cell_type":"markdown","metadata":{"id":"mLLUgji9OSKy"},"source":["### Define functions to train the model and evaluate results.\n","\n","\n"]},{"cell_type":"code","execution_count":194,"metadata":{"id":"tWLNHNEFOSKy","executionInfo":{"status":"ok","timestamp":1698501898774,"user_tz":-540,"elapsed":3,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[],"source":["import time\n","\n","\n","def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print(\n","                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n","                \"| accuracy {:8.3f}\".format(\n","                    epoch, idx, len(dataloader), total_acc / total_count\n","                )\n","            )\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc / total_count"]},{"cell_type":"markdown","metadata":{"id":"ePJQ3620OSKy"},"source":["### Split the dataset and run the model\n","\n","Since the original ``AG_NEWS`` has no valid dataset, we split the training\n","dataset into train/valid sets with a split ratio of 0.95 (train) and\n","0.05 (valid). Here we use\n","[torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)_\n","function in PyTorch core library.\n","\n","[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)_\n","criterion combines ``nn.LogSoftmax()`` and ``nn.NLLLoss()`` in a single class.\n","It is useful when training a classification problem with C classes.\n","[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)_\n","implements stochastic gradient descent method as the optimizer. The initial\n","learning rate is set to 5.0.\n","[StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)_\n","is used here to adjust the learning rate through epochs.\n","\n","\n"]},{"cell_type":"code","execution_count":195,"metadata":{"id":"leIPY_EMOSKy","outputId":"ed6b92cf-e5cf-455e-def0-146c107e503e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698502427833,"user_tz":-540,"elapsed":519776,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   500/ 1782 batches | accuracy    0.700\n","| epoch   1 |  1000/ 1782 batches | accuracy    0.872\n","| epoch   1 |  1500/ 1782 batches | accuracy    0.891\n","-----------------------------------------------------------\n","| end of epoch   1 | time: 10.49s | valid accuracy    0.906 \n","-----------------------------------------------------------\n","| epoch   2 |   500/ 1782 batches | accuracy    0.914\n","| epoch   2 |  1000/ 1782 batches | accuracy    0.921\n","| epoch   2 |  1500/ 1782 batches | accuracy    0.920\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 10.48s | valid accuracy    0.917 \n","-----------------------------------------------------------\n","| epoch   3 |   500/ 1782 batches | accuracy    0.936\n","| epoch   3 |  1000/ 1782 batches | accuracy    0.936\n","| epoch   3 |  1500/ 1782 batches | accuracy    0.935\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 10.45s | valid accuracy    0.920 \n","-----------------------------------------------------------\n","| epoch   4 |   500/ 1782 batches | accuracy    0.948\n","| epoch   4 |  1000/ 1782 batches | accuracy    0.946\n","| epoch   4 |  1500/ 1782 batches | accuracy    0.946\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 10.52s | valid accuracy    0.921 \n","-----------------------------------------------------------\n","| epoch   5 |   500/ 1782 batches | accuracy    0.956\n","| epoch   5 |  1000/ 1782 batches | accuracy    0.953\n","| epoch   5 |  1500/ 1782 batches | accuracy    0.957\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 10.27s | valid accuracy    0.925 \n","-----------------------------------------------------------\n","| epoch   6 |   500/ 1782 batches | accuracy    0.963\n","| epoch   6 |  1000/ 1782 batches | accuracy    0.962\n","| epoch   6 |  1500/ 1782 batches | accuracy    0.961\n","-----------------------------------------------------------\n","| end of epoch   6 | time: 10.13s | valid accuracy    0.926 \n","-----------------------------------------------------------\n","| epoch   7 |   500/ 1782 batches | accuracy    0.969\n","| epoch   7 |  1000/ 1782 batches | accuracy    0.968\n","| epoch   7 |  1500/ 1782 batches | accuracy    0.966\n","-----------------------------------------------------------\n","| end of epoch   7 | time: 10.12s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch   8 |   500/ 1782 batches | accuracy    0.976\n","| epoch   8 |  1000/ 1782 batches | accuracy    0.974\n","| epoch   8 |  1500/ 1782 batches | accuracy    0.975\n","-----------------------------------------------------------\n","| end of epoch   8 | time: 10.40s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch   9 |   500/ 1782 batches | accuracy    0.976\n","| epoch   9 |  1000/ 1782 batches | accuracy    0.975\n","| epoch   9 |  1500/ 1782 batches | accuracy    0.975\n","-----------------------------------------------------------\n","| end of epoch   9 | time: 10.40s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  10 |   500/ 1782 batches | accuracy    0.975\n","| epoch  10 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  10 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  10 | time: 10.49s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  11 |   500/ 1782 batches | accuracy    0.975\n","| epoch  11 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  11 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  11 | time: 10.29s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  12 |   500/ 1782 batches | accuracy    0.977\n","| epoch  12 |  1000/ 1782 batches | accuracy    0.975\n","| epoch  12 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  12 | time: 10.44s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  13 |   500/ 1782 batches | accuracy    0.975\n","| epoch  13 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  13 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  13 | time:  9.76s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  14 |   500/ 1782 batches | accuracy    0.975\n","| epoch  14 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  14 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  14 | time: 10.42s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  15 |   500/ 1782 batches | accuracy    0.977\n","| epoch  15 |  1000/ 1782 batches | accuracy    0.974\n","| epoch  15 |  1500/ 1782 batches | accuracy    0.975\n","-----------------------------------------------------------\n","| end of epoch  15 | time: 10.39s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  16 |   500/ 1782 batches | accuracy    0.976\n","| epoch  16 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  16 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  16 | time: 10.43s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  17 |   500/ 1782 batches | accuracy    0.976\n","| epoch  17 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  17 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  17 | time: 10.34s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  18 |   500/ 1782 batches | accuracy    0.977\n","| epoch  18 |  1000/ 1782 batches | accuracy    0.975\n","| epoch  18 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  18 | time: 10.46s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  19 |   500/ 1782 batches | accuracy    0.976\n","| epoch  19 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  19 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  19 | time: 10.12s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  20 |   500/ 1782 batches | accuracy    0.976\n","| epoch  20 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  20 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  20 | time: 10.02s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  21 |   500/ 1782 batches | accuracy    0.976\n","| epoch  21 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  21 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  21 | time: 10.60s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  22 |   500/ 1782 batches | accuracy    0.976\n","| epoch  22 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  22 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  22 | time: 10.30s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  23 |   500/ 1782 batches | accuracy    0.976\n","| epoch  23 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  23 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  23 | time: 10.39s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  24 |   500/ 1782 batches | accuracy    0.975\n","| epoch  24 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  24 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  24 | time: 10.47s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  25 |   500/ 1782 batches | accuracy    0.976\n","| epoch  25 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  25 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  25 | time: 10.52s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  26 |   500/ 1782 batches | accuracy    0.976\n","| epoch  26 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  26 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  26 | time:  9.79s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  27 |   500/ 1782 batches | accuracy    0.976\n","| epoch  27 |  1000/ 1782 batches | accuracy    0.975\n","| epoch  27 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  27 | time: 10.40s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  28 |   500/ 1782 batches | accuracy    0.976\n","| epoch  28 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  28 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  28 | time: 10.26s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  29 |   500/ 1782 batches | accuracy    0.975\n","| epoch  29 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  29 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  29 | time: 10.42s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  30 |   500/ 1782 batches | accuracy    0.976\n","| epoch  30 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  30 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  30 | time: 10.42s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  31 |   500/ 1782 batches | accuracy    0.975\n","| epoch  31 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  31 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  31 | time: 11.32s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  32 |   500/ 1782 batches | accuracy    0.974\n","| epoch  32 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  32 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  32 | time:  9.99s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  33 |   500/ 1782 batches | accuracy    0.976\n","| epoch  33 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  33 |  1500/ 1782 batches | accuracy    0.975\n","-----------------------------------------------------------\n","| end of epoch  33 | time: 10.22s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  34 |   500/ 1782 batches | accuracy    0.977\n","| epoch  34 |  1000/ 1782 batches | accuracy    0.975\n","| epoch  34 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  34 | time: 10.28s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  35 |   500/ 1782 batches | accuracy    0.976\n","| epoch  35 |  1000/ 1782 batches | accuracy    0.974\n","| epoch  35 |  1500/ 1782 batches | accuracy    0.978\n","-----------------------------------------------------------\n","| end of epoch  35 | time: 10.32s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  36 |   500/ 1782 batches | accuracy    0.977\n","| epoch  36 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  36 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  36 | time: 10.45s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  37 |   500/ 1782 batches | accuracy    0.975\n","| epoch  37 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  37 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  37 | time: 10.56s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  38 |   500/ 1782 batches | accuracy    0.976\n","| epoch  38 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  38 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  38 | time: 10.79s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  39 |   500/ 1782 batches | accuracy    0.976\n","| epoch  39 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  39 |  1500/ 1782 batches | accuracy    0.975\n","-----------------------------------------------------------\n","| end of epoch  39 | time: 10.14s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  40 |   500/ 1782 batches | accuracy    0.975\n","| epoch  40 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  40 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  40 | time: 10.48s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  41 |   500/ 1782 batches | accuracy    0.974\n","| epoch  41 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  41 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  41 | time: 10.59s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  42 |   500/ 1782 batches | accuracy    0.975\n","| epoch  42 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  42 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  42 | time: 10.55s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  43 |   500/ 1782 batches | accuracy    0.976\n","| epoch  43 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  43 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  43 | time: 10.38s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  44 |   500/ 1782 batches | accuracy    0.976\n","| epoch  44 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  44 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  44 | time: 10.37s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  45 |   500/ 1782 batches | accuracy    0.974\n","| epoch  45 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  45 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  45 | time: 10.45s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  46 |   500/ 1782 batches | accuracy    0.977\n","| epoch  46 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  46 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  46 | time:  9.70s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  47 |   500/ 1782 batches | accuracy    0.977\n","| epoch  47 |  1000/ 1782 batches | accuracy    0.976\n","| epoch  47 |  1500/ 1782 batches | accuracy    0.975\n","-----------------------------------------------------------\n","| end of epoch  47 | time: 10.45s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  48 |   500/ 1782 batches | accuracy    0.976\n","| epoch  48 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  48 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  48 | time: 10.36s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  49 |   500/ 1782 batches | accuracy    0.976\n","| epoch  49 |  1000/ 1782 batches | accuracy    0.977\n","| epoch  49 |  1500/ 1782 batches | accuracy    0.976\n","-----------------------------------------------------------\n","| end of epoch  49 | time: 10.45s | valid accuracy    0.923 \n","-----------------------------------------------------------\n","| epoch  50 |   500/ 1782 batches | accuracy    0.975\n","| epoch  50 |  1000/ 1782 batches | accuracy    0.978\n","| epoch  50 |  1500/ 1782 batches | accuracy    0.977\n","-----------------------------------------------------------\n","| end of epoch  50 | time: 10.44s | valid accuracy    0.923 \n","-----------------------------------------------------------\n"]}],"source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","\n","# Hyperparameters\n","EPOCHS = 50  # epoch\n","LR = 0.001  # learning rate\n","BATCH_SIZE = 64  # batch size for training\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","train_iter, test_iter = AG_NEWS()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = random_split(\n","    train_dataset, [num_train, len(train_dataset) - num_train]\n",")\n","\n","train_dataloader = DataLoader(\n","    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",")\n","valid_dataloader = DataLoader(\n","    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",")\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","        scheduler.step()\n","    else:\n","        total_accu = accu_val\n","    print(\"-\" * 59)\n","    print(\n","        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n","        \"valid accuracy {:8.3f} \".format(\n","            epoch, time.time() - epoch_start_time, accu_val\n","        )\n","    )\n","    print(\"-\" * 59)"]},{"cell_type":"markdown","metadata":{"id":"Ck_IFJpGOSKy"},"source":["### Evaluate the model with test dataset\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vklsJMsEOSKy"},"source":["Checking the results of the test dataset…\n","\n"]},{"cell_type":"code","execution_count":196,"metadata":{"id":"H5hsAElDOSKy","outputId":"91719489-6aac-4f0e-8e40-28de3c42e33c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698502428381,"user_tz":-540,"elapsed":557,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking the results of test dataset.\n","test accuracy    0.918\n"]}],"source":["print(\"Checking the results of test dataset.\")\n","accu_test = evaluate(test_dataloader)\n","print(\"test accuracy {:8.3f}\".format(accu_test))"]},{"cell_type":"markdown","metadata":{"id":"avnJmgDnOSKz"},"source":["### Test on a random news\n","\n","Use the best model so far and test a golf news.\n","\n","\n"]},{"cell_type":"code","execution_count":197,"metadata":{"id":"ywiyjhVNOSKz","outputId":"a0b5367f-5bc6-43ae-ccf9-3fd8389c523d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698502428382,"user_tz":-540,"elapsed":4,"user":{"displayName":"컴퓨터교육과/박병준","userId":"16195189156410273961"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["This is a Sports news\n"]}],"source":["ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n","\n","\n","def predict(text, text_pipeline):\n","    with torch.no_grad():\n","        text = torch.tensor(text_pipeline(text))\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","\n","ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","model = model.to(\"cpu\")\n","\n","print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"1Qwp8A1B95PIrizmmflG4mvpirBzFLAA1","timestamp":1698204427727}],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}